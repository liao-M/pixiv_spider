最开始写这个爬虫是觉得每次自己去p站扒图好蠢，于是想自己写一个。

只是可惜我的python水平实在抠脚，写了两个下午才写了个雏形出来，借鉴了不少网上的经验，比如下载原图时要加入的referer.下载速度很慢，而且暂时只能下载单图，正在考（xue）虑（xi）加入多线程。

我的想法是除了第一次下载所有图片外，以后每次都只下载新出现的作品。我的基本想法是，每次先爬有多少作品，保存在一个txt里，然后每次打开爬虫的时候进行对比就好了，可是我发现照我这种每个页面爬一遍然后统计总数的方法来做的话实在太慢了。。。但是坑爹的p站没有尾页这一说法，我暂时也想不出来什么好办法。

目前的代码也很丑，乱得很，下次写多线程的时候重构（写）一下吧

暂时就这么写吧，好蠢(｡ŏ_ŏ)

------------------16/9/29分割--------------------


关于只下载新图的方法，我有了一种新的设想，由于我保存的时候是用的作品名字，那么每次只下载动态页的前几页就好了，因为新的图片只会更新在动态页的前端，不过这个想法好讨巧（逃

-----------------16/12/23分割--------------------
闲置了一段实时间没有再弄，这次在原来的基础上加入了多图处理，主要思路是：多图的链接里用原来匹配original-image的语法匹配不上：

 `soup.find_all("img", attrs={"class": "original-image"})`

 据此判断多图，而多图界面的链接就是把原链接里的`medium&amp`段置换为`manga&`就好了

另外还在这个爬虫的基础上又写了一个，输入id 抓取该作者所有作品，以作者名字保存为一个单独的文件夹，没什么难度，稍微改一改就好了，也许有一天我会把这几个功能合并吧。

#### todo list

- [x] 多线程
- [ ] 错误处理
- [ ] 图形界面
- [x] 处理多图
